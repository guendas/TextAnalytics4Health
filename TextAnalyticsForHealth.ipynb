{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "#conda activate text-analytics\r\n",
    "\r\n",
    "import os\r\n",
    "import json as js\r\n",
    "import requests, uuid\r\n",
    "import re, math\r\n",
    "import PyPDF2\r\n",
    "import fitz\r\n",
    "from datetime import datetime, timedelta\r\n",
    "from docx import Document\r\n",
    "from docx.enum.text import WD_COLOR_INDEX\r\n",
    "from azure.core.credentials import AzureKeyCredential\r\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
    "from azure.ai.translation.document import DocumentTranslationClient\r\n",
    "from azure.storage.blob import BlobServiceClient\r\n",
    "from azure.storage.blob._shared_access_signature import BlobSharedAccessSignature\r\n",
    "\r\n",
    "#Loads info from config file \r\n",
    "with open('./config.json','r') as file:\r\n",
    "    config = js.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Demo setup </h1>\r\n",
    "\r\n",
    "Before running this demo you need few things set up: \r\n",
    "\r\n",
    "<h4>Azure Resources</h4>\r\n",
    "<ul>\r\n",
    "<li> Storage Account, in the blob storage create two containers: </li>\r\n",
    "<ul>\r\n",
    "<li> data </li>\r\n",
    "<li> output </li>\r\n",
    "</ul>\r\n",
    "<li> Azure Text Analytics: https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/overview </li>\r\n",
    "<li> Azure Translator: https://docs.microsoft.com/en-us/azure/cognitive-services/translator/document-translation/get-started-with-document-translation?WT.mc_id=Portal-Microsoft_Azure_ProjectOxford&tabs=csharp </li>\r\n",
    "<ul>\r\n",
    "<li>Create a SAS token policy to use Document Translator here: https://docs.microsoft.com/en-us/azure/cognitive-services/translator/document-translation/create-sas-tokens?tabs=Containers</li>\r\n",
    "</ul>\r\n",
    "</ul>\r\n",
    "\r\n",
    "<h4> Folder Structure </h4>\r\n",
    "In the root folder:\r\n",
    "<ul>\r\n",
    "<li> \"data\" folder, inside the data folder: </li>\r\n",
    "<ul>\r\n",
    "<li> \"original\" folder (where your original pdf files will be)</li>\r\n",
    "<li> \"output\" folder (where your outputted highlighted docx file will be stored)</li>\r\n",
    "<li> \"preprocessed\" folder (where your docx file will be)</li>\r\n",
    "<li> \"translated\" folder (where you can download your translated docx)</li>\r\n",
    "</ul>\r\n",
    "<li> \"config.json\" file, with all the path and keys </li>\r\n",
    "<li> notebook </li>\r\n",
    "</ul>\r\n",
    "\r\n",
    "**N.B. Edit variable <em>container_name</em> with the name of your container.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Blob functions helpers </h2>\r\n",
    "\r\n",
    "Helper functions to run basic operations on Azure Blob Storage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Blob helpers\r\n",
    "\r\n",
    "######################################\r\n",
    "# Creates connection to blob storage #\r\n",
    "######################################\r\n",
    "def ConnectToBlobStorage(connectionString):\r\n",
    "    #connects to the Azure Storage Account\r\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connectionString)\r\n",
    "    return blob_service_client\r\n",
    "\r\n",
    "###################################\r\n",
    "# Lists container in blob storage #\r\n",
    "###################################\r\n",
    "def GetContainersInStorage(blob_service_client):\r\n",
    "    container_list = blob_service_client.list_containers()\r\n",
    "    return container_list\r\n",
    "\r\n",
    "#####################################\r\n",
    "# Lists blobs in specific container #\r\n",
    "#####################################\r\n",
    "def GetBlobsInContainer(blobClient,containerName):\r\n",
    "    #gets the container you want to list\r\n",
    "    container_client = blobClient.get_container_client(containerName)\r\n",
    "    #gets the blobs\r\n",
    "    blobs_list = container_client.list_blobs()\r\n",
    "    return blobs_list\r\n",
    "\r\n",
    "###############################################\r\n",
    "# Downloads locally specific blob in storage  #\r\n",
    "###############################################\r\n",
    "def DownloadBlobLocally(blobServiceClient,containerName,fileName,downloadPath):\r\n",
    "    #creates blob client\r\n",
    "    blob_client = blobServiceClient.get_blob_client(container=containerName,blob=fileName)\r\n",
    "    print(\"Downloading from Azure Storage as blob: \" + fileName)\r\n",
    "    #downloads file\r\n",
    "    download_file_path= downloadPath + fileName\r\n",
    "    with open(download_file_path,\"wb\") as download_file:\r\n",
    "        download_file.write(blob_client.download_blob().readall())\r\n",
    "    print(\"Blob downloaded in the following folder: \"+download_file_path)\r\n",
    "    return download_file_path\r\n",
    "\r\n",
    "########################################################\r\n",
    "# Uploads local file to specific container in storage  #\r\n",
    "########################################################\r\n",
    "def UploadFileToBlob(blob_service_client,containerName,fileName,filePath):\r\n",
    "    #creates blob client\r\n",
    "    blob_client = blob_service_client.get_blob_client(container=containerName,blob=fileName)  \r\n",
    "    print(\"Uploading to Azure Storage as blob: \" + fileName)\r\n",
    "    #uploads file\r\n",
    "    with open(filePath,\"rb\") as upload_file:\r\n",
    "        blob_client.upload_blob(upload_file)\r\n",
    "    print(\"Blob uploaded!\")\r\n",
    "\r\n",
    "##########################################\r\n",
    "# Gets blob url from specific container  #\r\n",
    "##########################################\r\n",
    "def GetBlobURL(blob_service_client,blob_name,container_name):\r\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name,blob=blob_name)\r\n",
    "    blob_url = blob_client.url\r\n",
    "    return blob_url\r\n",
    "\r\n",
    "###########################################\r\n",
    "# Creates SAS signature for specific blob #\r\n",
    "###########################################\r\n",
    "def CreateSASSignature(container_name, blob_name, permissions,expiry):\r\n",
    "    blob_shared_access_signature = BlobSharedAccessSignature(config[\"sa_account_name\"],config[\"sa_key\"])\r\n",
    "    sas_token = blob_shared_access_signature.generate_blob(container_name,blob_name,expiry=expiry,permission=\"rw\")\r\n",
    "    return sas_token"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Local function helpers </h2>\r\n",
    "\r\n",
    "Helper functions to run basic operations on local files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Local helpers\r\n",
    "\r\n",
    "################################\r\n",
    "# Lists files in folder        #\r\n",
    "################################\r\n",
    "def FilesInFolder(path):\r\n",
    "    file_list = os.listdir(path)\r\n",
    "    return file_list\r\n",
    "\r\n",
    "################################\r\n",
    "#Extracts text from docx file  #\r\n",
    "################################\r\n",
    "def ExtractTextFromLocal(docPath):\r\n",
    "    doc = Document(docPath)\r\n",
    "    full_text = []\r\n",
    "    for t in doc.paragraphs:\r\n",
    "        full_text.append(t.text)\r\n",
    "    return full_text\r\n",
    "\r\n",
    "##############################################\r\n",
    "# Highlights entities found in the docx file #\r\n",
    "##############################################\r\n",
    "def EntitySearch(document,keyword,dstPath,color):\r\n",
    "    for p in document.paragraphs:\r\n",
    "        if keyword in p.text:\r\n",
    "            for run in p.runs:\r\n",
    "                if keyword in run.text:\r\n",
    "                    temp = run.text.split(keyword)\r\n",
    "                    run.clear()\r\n",
    "                    for i in range(len(temp)-1):\r\n",
    "                        #run.add_text(temp[i])\r\n",
    "                        run.add_text(keyword)\r\n",
    "                        if(color == 'y'):\r\n",
    "                            run.font.highlight_color = WD_COLOR_INDEX.YELLOW\r\n",
    "                        else:\r\n",
    "                            run.font.highlight_color = WD_COLOR_INDEX.GREEN\r\n",
    "    document.save(dstPath)\r\n",
    "\r\n",
    "################################################\r\n",
    "# Takes extracted document text and divides it #\r\n",
    "# in max n parts to avoid API limits           #\r\n",
    "################################################\r\n",
    "def ChunkText(text,n):\r\n",
    "    max_size = math.ceil(len(text)/n)\r\n",
    "    max_length = len(text)-1\r\n",
    "    chunks_text = []\r\n",
    "    for x in range(0,max_length,max_size):\r\n",
    "        temp = \"\"\r\n",
    "        for t in range(x,x+max_size):\r\n",
    "            if(t >= max_length): \r\n",
    "                break \r\n",
    "            temp = temp + \" \"+ text[t]   \r\n",
    "        chunks_text.append(temp)\r\n",
    "    return chunks_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Text Analytics function helpers </h2>\r\n",
    "\r\n",
    "Helper functions to run basic operations using Azure Text Analytics API."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Text Analytics helpers\r\n",
    "\r\n",
    "#############################\r\n",
    "# Detects document language #\r\n",
    "#############################\r\n",
    "def LanguageDetection(text_analytics_client,full_text):\r\n",
    "    language_detected = text_analytics_client.detect_language(full_text)\r\n",
    "    print(\"Language detected: {}\".format(language_detected[0].primary_language))\r\n",
    "    return language_detected\r\n",
    "\r\n",
    "############################################\r\n",
    "# Translates documents online (from blob)  #\r\n",
    "############################################\r\n",
    "def DocumentTranslation(transaltor_client,source_url,target_url,language):\r\n",
    "    poller = transaltor_client.begin_translation(source_url,target_url,language)\r\n",
    "    result = poller.result()\r\n",
    "    #print(\"Status: {}\".format(poller.status()))\r\n",
    "    #print(\"Created on: {}\".format(poller.details.created_on))\r\n",
    "    #print(\"Last updated on: {}\".format(poller.details.last_updated_on))\r\n",
    "    #print(\"Total number of translations on documents: {}\".format(poller.details.documents_total_count))\r\n",
    "    for document in result:\r\n",
    "        print(\"Document ID: {}\".format(document.id))\r\n",
    "        print(\"Document status: {}\".format(document.status))\r\n",
    "        if document.status == \"Succeeded\":\r\n",
    "            print(\"Source document location: {}\".format(document.source_document_url))\r\n",
    "            print(\"Translated document location: {}\".format(document.translated_document_url))\r\n",
    "            print(\"Translated to language: {}\\n\".format(document.translated_to))\r\n",
    "            return document.translated_document_url\r\n",
    "        else:\r\n",
    "            print(\"Error Code: {}, Message: {}\\n\".format(document.error.code, document.error.message))\r\n",
    "\r\n",
    "#######################################\r\n",
    "# Extracts health entities from text  #\r\n",
    "#######################################\r\n",
    "def TextAnalyticsForHealth(text_analytics_client,text):\r\n",
    "    poller = text_analytics_client.begin_analyze_healthcare_entities(text)\r\n",
    "    result = poller.result()\r\n",
    "    entities = []\r\n",
    "    docs = [doc for doc in result if not doc.is_error]\r\n",
    "    print(\"Healthcare results:\")\r\n",
    "    for idx, doc in enumerate(docs):\r\n",
    "        for entity in doc.entities:\r\n",
    "            entities.append(entity.text)\r\n",
    "    #         print(\"Entity: {}\".format(entity.text))\r\n",
    "    #         print(\"...Normalized Text: {}\".format(entity.normalized_text))\r\n",
    "    #         print(\"...Category: {}\".format(entity.category))\r\n",
    "    #         print(\"...Subcategory: {}\".format(entity.subcategory))\r\n",
    "    #         print(\"...Offset: {}\".format(entity.offset))\r\n",
    "    #         print(\"...Confidence score: {}\".format(entity.confidence_score))\r\n",
    "    #         if entity.data_sources is not None:\r\n",
    "    #             print(\"...Data Sources:\")\r\n",
    "    #             for data_source in entity.data_sources:\r\n",
    "    #                 print(\"......Entity ID: {}\".format(data_source.entity_id))\r\n",
    "    #                 print(\"......Name: {}\".format(data_source.name))\r\n",
    "    #         if entity.assertion is not None:\r\n",
    "    #             print(\"...Assertion:\")\r\n",
    "    #             print(\"......Conditionality: {}\".format(entity.assertion.conditionality))\r\n",
    "    #             print(\"......Certainty: {}\".format(entity.assertion.certainty))\r\n",
    "    #             print(\"......Association: {}\".format(entity.assertion.association))\r\n",
    "    #     for relation in doc.entity_relations:\r\n",
    "    #         print(\"Relation of type: {} has the following roles\".format(relation.relation_type))\r\n",
    "    #         for role in relation.roles:\r\n",
    "    #             print(\"...Role '{}' with entity '{}'\".format(role.name, role.entity.text))\r\n",
    "    #     print(\"------------------------------------------\")\r\n",
    "    return docs,entities\r\n",
    "\r\n",
    "###########################\r\n",
    "# Extracts PII from text  #\r\n",
    "###########################\r\n",
    "def TextAnalyticsPII(text_analytics_client,text,language):\r\n",
    "    response = text_analytics_client.recognize_pii_entities(text, language=language)\r\n",
    "    result = [doc for doc in response if not doc.is_error]\r\n",
    "    pii_entities = []\r\n",
    "    for idx, doc in enumerate(result):\r\n",
    "        #print(\"Document text: {}\".format(documents[idx]))\r\n",
    "        #print(\"Redacted document text: {}\".format(doc.redacted_text))\r\n",
    "        for entity in doc.entities:\r\n",
    "            pii_entities.append(entity.text)\r\n",
    "            #print(\"...Entity: {}\".format(entity.text))\r\n",
    "            #print(\"......Category: {}\".format(entity.category))\r\n",
    "            #print(\"......Confidence Score: {}\".format(entity.confidence_score))\r\n",
    "            #print(\"......Offset: {}\".format(entity.offset))\r\n",
    "    return result,pii_entities\r\n",
    "\r\n",
    "#############################################\r\n",
    "# Execute all the steps to: extracts text   #\r\n",
    "# from docx, translates it, extracts health #\r\n",
    "#    entities, highlights them in docx,     #\r\n",
    "#    extracts PII entities from docx        #\r\n",
    "#############################################\r\n",
    "def TextAnalyticsOnDocs(docs_folder,translated_folder,output_folder,ta_client,tr_client,blob_client,container_name,source_url,target_url):\r\n",
    "    #Check files locally \r\n",
    "    file_list = FilesInFolder(docs_folder)\r\n",
    "    for f in file_list:\r\n",
    "        print(\"File processed {}\".format(f))\r\n",
    "        #Stores file path\r\n",
    "        file_path = docs_folder + f\r\n",
    "        #Extract file text\r\n",
    "        extracted_text = ExtractTextFromLocal(file_path)\r\n",
    "        ########################\r\n",
    "        #       STEP 1         #\r\n",
    "        #                      #\r\n",
    "        #  LANGUAGE DETECTION  #\r\n",
    "        ########################\r\n",
    "        language_detected = LanguageDetection(ta_client,extracted_text)\r\n",
    "        ########################\r\n",
    "        #       STEP 2         #\r\n",
    "        #                      #\r\n",
    "        # DOCUMENT TRANSLATION #\r\n",
    "        ########################\r\n",
    "        #Upload file to Blob storage\r\n",
    "        UploadFileToBlob(blob_client,container_name,f,file_path)\r\n",
    "        #Translate document\r\n",
    "        translated_doc_url = DocumentTranslation(tr_client,source_url,target_url,\"en\")\r\n",
    "        #################################\r\n",
    "        #             STEP 3            #\r\n",
    "        #                               #\r\n",
    "        # DOWNLOAD AND EXTRACT ENTITIES #\r\n",
    "        #################################\r\n",
    "        #Download file \r\n",
    "        translated_file_path = DownloadBlobLocally(blob_client,\"output\",f,translated_folder)\r\n",
    "        fully_translated_text = ExtractTextFromLocal(translated_file_path)\r\n",
    "        #Format text to be sent to the TA\r\n",
    "        ct = ChunkText(fully_translated_text,10)\r\n",
    "        #Extract entities\r\n",
    "        entities, health_entities = TextAnalyticsForHealth(ta_client,ct)\r\n",
    "        #################################\r\n",
    "        #             STEP 4            #\r\n",
    "        #                               #\r\n",
    "        #   HIGHLIGHT ENTITIES IN DOCX  #\r\n",
    "        #################################\r\n",
    "        translated_document = Document(translated_file_path)\r\n",
    "        #Highlighted doc path\r\n",
    "        highlighted_doc_path = output_folder + f\r\n",
    "        for d in entities:\r\n",
    "            for entity in d.entities:\r\n",
    "                print(entity.text)\r\n",
    "                EntitySearch(translated_document,entity.text,highlighted_doc_path,'y')\r\n",
    "        #################################\r\n",
    "        #             STEP 5            #\r\n",
    "        #                               #\r\n",
    "        #              PII              #\r\n",
    "        #################################\r\n",
    "        pii_extraction = ChunkText(fully_translated_text,5)\r\n",
    "        results,pii_entities = TextAnalyticsPII(ta_client,pii_extraction,\"en\")\r\n",
    "        #################################\r\n",
    "        #             STEP 6            #\r\n",
    "        #                               #\r\n",
    "        #         HIGHLIGHT PII         #\r\n",
    "        #################################\r\n",
    "        for r in results:\r\n",
    "            for entity in r.entities:\r\n",
    "                #Uncomment to show results\r\n",
    "                #print(entity.text)\r\n",
    "                EntitySearch(translated_document,entity.text,highlighted_doc_path,'g')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>User input functions</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "###########################################\r\n",
    "#    Print user choice on what kind       #\r\n",
    "#    of operation wants to execute        #\r\n",
    "###########################################\r\n",
    "\r\n",
    "def UserActivityChoice():\r\n",
    "    print(\"\"\" Hi, what do you whant to do? \\n \r\n",
    "        1. Read PDF and extract text\r\n",
    "        2. Detect PDF language \r\n",
    "        3. Translate PDF \r\n",
    "        4. Extract entities\r\n",
    "        5. Extract PII \r\n",
    "        6. All the above \r\n",
    "        Digit 'exit' to close the prompt \\n\"\"\")\r\n",
    "\r\n",
    "    user_choice = input()   \r\n",
    "    return(user_choice)\r\n",
    "\r\n",
    "#####################################\r\n",
    "#    Print user choice on which     #\r\n",
    "#    file wants to execute the      #\r\n",
    "#    operation on                   #\r\n",
    "#####################################\r\n",
    "def UserFileChoice(original_folder):\r\n",
    "    files = FilesInFolder(original_folder)\r\n",
    "    count_files = 1\r\n",
    "    for f in files:\r\n",
    "            print(\"{}. {}\".format(count_files,f))\r\n",
    "            count_files = count_files + 1\r\n",
    "    print(\"{}. All the above \\n\".format(count_files))\r\n",
    "    file_choice = input()\r\n",
    "    num_docs = len(files)\r\n",
    "    return file_choice,num_docs,files\r\n",
    "    \r\n",
    "################################################\r\n",
    "#       Reads a pdf file and extract text      #\r\n",
    "################################################\r\n",
    "\r\n",
    "def ReadPDFandExtractText(file_path):\r\n",
    "    pdf = PyPDF2.PdfFileReader(file_path)\r\n",
    "    #Gets PDF page number\r\n",
    "    page_num = pdf.getNumPages()\r\n",
    "    pdf_text = \"\"\r\n",
    "    if(page_num == 0):\r\n",
    "        print(\"PDF is empty\")\r\n",
    "        return pdf_text\r\n",
    "    else:\r\n",
    "        for p in range(page_num):\r\n",
    "            pdf_text = pdf_text + pdf.getPage(p).extractText()\r\n",
    "        pdf_text = pdf_text.split('\\n')\r\n",
    "        return pdf_text\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Text Analytics </h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Variables </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#Helpers\r\n",
    "DOCS_FOLDER = config[\"preprocessed_path\"]\r\n",
    "ORIGINAL_FOLDER =config[\"pdfs_path\"]\r\n",
    "OUTPUT_FOLDER = config[\"output_path\"]\r\n",
    "TRANSLATED_FOLDER = config[\"translated_path\"]\r\n",
    "SOURCE_URL = config[\"sa_source_url\"]\r\n",
    "TARGET_URL = config[\"sa_target_url\"]\r\n",
    "NUM_MAX_DOCS = 0\r\n",
    "\r\n",
    "#Connect to Azure Text Analytics service\r\n",
    "ta_credentials = AzureKeyCredential(config[\"text_analytics_key\"])\r\n",
    "TEXT_ANALYTICS_CLIENT = TextAnalyticsClient(endpoint=config[\"text_analytics_endpoint\"],credential=ta_credentials)\r\n",
    "\r\n",
    "#Connect to Azure Translator service\r\n",
    "tr_credentials = AzureKeyCredential(config[\"translator_key\"])\r\n",
    "TRANSLATOR_CLIENT= DocumentTranslationClient(endpoint=config[\"translator_documents_endpoint\"],credential=tr_credentials)\r\n",
    "\r\n",
    "#Blob storage connection\r\n",
    "BLOBSERVICECLIENT = ConnectToBlobStorage(config[\"sa_connectionstring\"])\r\n",
    "#Hardcoded container name\r\n",
    "CONTAINER_NAME_UP = \"data\"\r\n",
    "CONTAINER_NAME_DOWN = \"output\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Prompts for user input </h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "user_choice = UserActivityChoice()\r\n",
    "file_choice,NUM_MAX_DOCS,files = UserFileChoice(ORIGINAL_FOLDER)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Hi, what do you whant to do? \n",
      " \n",
      "        1. Read PDF and extract text\n",
      "        2. Detect PDF language \n",
      "        3. Translate PDF \n",
      "        4. Extract entities\n",
      "        5. Extract PII \n",
      "        6. All the above \n",
      "        Digit 'exit' to close the prompt \n",
      "\n",
      "1. LDO_AOPR_1.pdf\n",
      "2. LDO_AOPR_2.pdf\n",
      "3. LDO_AUSBO_1.pdf\n",
      "4. LDO_AUSBO_2.pdf\n",
      "5. LDO_AUSBO_3.pdf\n",
      "6. LDO_AUSBO_4.pdf\n",
      "7. LDO_AUSBO_5.pdf\n",
      "8. LDO_AVEC_1.pdf\n",
      "9. LDO_IOR_1.pdf\n",
      "10. LDO_IOR_2.pdf\n",
      "11. LDO_IOR_3.pdf\n",
      "12. VPS_AUSBO_1.pdf\n",
      "13. VPS_AUSBO_2.pdf\n",
      "14. VPS_AUSBO_3.pdf\n",
      "15. VPS_IOR_1.pdf\n",
      "16. VPS_IOR_2.pdf\n",
      "17. All the above \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "############################################\r\n",
    "#          Recap of user choices           # \r\n",
    "############################################\r\n",
    "if(int(file_choice) >= NUM_MAX_DOCS):\r\n",
    "    print(\"You choose: {} on all docs. Starting execution. \".format(user_choice))\r\n",
    "else:\r\n",
    "    print(\"You choose: {} on {}. Starting execution. \".format(user_choice,files[int(file_choice)-1]))\r\n",
    "\r\n",
    "#SelectionSwitch(user_choice,file_choice,files)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You choose: 3 on LDO_AUSBO_2.pdf. Starting execution. \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "user_choice_num = int(user_choice)\r\n",
    "file_choice_num = int(file_choice)\r\n",
    "\r\n",
    "pdf_text = \"\"\r\n",
    "translated_text = \"\"\r\n",
    "\r\n",
    "if(user_choice_num == 1):\r\n",
    "    ##########################\r\n",
    "    # Read and extract file  #\r\n",
    "    ##########################\r\n",
    "    if(file_choice_num == NUM_MAX_DOCS):\r\n",
    "        print(\"Executing on all documents\")\r\n",
    "    else:\r\n",
    "        print(\"Reading and extracting text from {}\".format(files[file_choice_num-1]))\r\n",
    "        pdf_text = ReadPDFandExtractText(ORIGINAL_FOLDER+files[file_choice_num-1])\r\n",
    "elif(user_choice_num == 2):\r\n",
    "    ##########################\r\n",
    "    #  Detect PDF language   #\r\n",
    "    ##########################\r\n",
    "    if(file_choice_num == NUM_MAX_DOCS):\r\n",
    "        print(\"Executing on all documents\")\r\n",
    "    else:\r\n",
    "        print(\"Detecting language from {}\".format(files[file_choice_num-1]))\r\n",
    "        if(pdf_text == \"\"):\r\n",
    "            pdf_text = ReadPDFandExtractText(ORIGINAL_FOLDER+files[file_choice_num-1])\r\n",
    "        else:\r\n",
    "            language_detected = LanguageDetection(TEXT_ANALYTICS_CLIENT,pdf_text)\r\n",
    "elif(user_choice_num == 3):\r\n",
    "    ##########################\r\n",
    "    #  Translate online PDF  #\r\n",
    "    ##########################\r\n",
    "    print(\"Which is the translation language?(it or en)\")\r\n",
    "    lang_translation = input()\r\n",
    "    if(file_choice_num == NUM_MAX_DOCS):\r\n",
    "        print(\"Executing on all documents\")\r\n",
    "    else:\r\n",
    "        print(\"Translating {} document\".format(files[file_choice_num-1]))\r\n",
    "        #Uploads document to Azure Blob Storage\r\n",
    "        UploadFileToBlob(BLOBSERVICECLIENT,CONTAINER_NAME_UP,files[file_choice_num-1],ORIGINAL_FOLDER+files[file_choice_num-1])\r\n",
    "        if(lang_translation == 'en'):\r\n",
    "            doc_url = DocumentTranslation(TRANSLATOR_CLIENT,SOURCE_URL,TARGET_URL,lang_translation)                    \r\n",
    "        elif(lang_translation == 'it'):\r\n",
    "            print(\"\")\r\n",
    "elif(user_choice_num == 4):\r\n",
    "    ##########################\r\n",
    "    #   Extract entities     #\r\n",
    "    ##########################\r\n",
    "    if(file_choice_num == NUM_MAX_DOCS):\r\n",
    "        print(\"Executing on all documents\")\r\n",
    "    else:\r\n",
    "        print(\"Extracting entities from {}\".format(files[file_choice_num-1]))\r\n",
    "        translated_file_path = DownloadBlobLocally(BLOBSERVICECLIENT,CONTAINER_NAME_DOWN,files[file_choice_num-1],TRANSLATED_FOLDER)\r\n",
    "        translated_text = ReadPDFandExtractText(ORIGINAL_FOLDER+files[file_choice_num-1])\r\n",
    "        translated_text_formatted = ChunkText(translated_text,10)\r\n",
    "        entities, health_entities = TextAnalyticsForHealth(TEXT_ANALYTICS_CLIENT,translated_text_formatted)\r\n",
    "elif(user_choice_num == 5):\r\n",
    "    ##########################\r\n",
    "    #       Extract PII      #\r\n",
    "    ##########################\r\n",
    "    if(file_choice_num == NUM_MAX_DOCS):\r\n",
    "        print(\"Executing on all documents\")\r\n",
    "    else:\r\n",
    "        print(\"Extracting PII from {}\".format(files[file_choice_num-1]))\r\n",
    "        if(translated_text == \"\"):\r\n",
    "            translated_file_path = DownloadBlobLocally(BLOBSERVICECLIENT,CONTAINER_NAME_DOWN,files[file_choice_num-1],TRANSLATED_FOLDER)\r\n",
    "            translated_text = ReadPDFandExtractText(ORIGINAL_FOLDER+files[file_choice_num-1])\r\n",
    "        else:\r\n",
    "            translated_text_pii = ChunkText(translated_text,5)\r\n",
    "            pii_results,pii_entities = TextAnalyticsPII(TEXT_ANALYTICS_CLIENT,translated_text_pii,\"en\")\r\n",
    "elif(user_choice_num == 6):\r\n",
    "    ##########################\r\n",
    "    #      All the above     #\r\n",
    "    #     TO BE DONE         #\r\n",
    "    ##########################\r\n",
    "    if(file_choice_num == NUM_MAX_DOCS):\r\n",
    "        print(\"Executing on all documents\")\r\n",
    "    else:\r\n",
    "        print(\"Executing all operation on {}\".format(files[file_choice_num-1]))\r\n",
    "        #Reads the PDF\r\n",
    "        pdf_text = ReadPDFandExtractText(ORIGINAL_FOLDER+files[file_choice_num-1])\r\n",
    "        #Formats the PDF for extraction\r\n",
    "        pdf_text_formatted = ChunkText(pdf_text,10)\r\n",
    "        #Detect PDF language\r\n",
    "        lang = LanguageDetection(TEXT_ANALYTICS_CLIENT,pdf_text)\r\n",
    "        #Translate online PDF      \r\n",
    "else:\r\n",
    "    if(user_choice == 'exit'):\r\n",
    "        print(\"Closing the prompt\")\r\n",
    "    else:\r\n",
    "        print(\"No choice selected. {}\".format(user_choice_num))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting entities from LDO_AUSBO_2.pdf\n",
      "Downloading from Azure Storage as blob: LDO_AUSBO_2.pdf\n",
      "Blob downloaded in the following folder: C://Users//guscianc//source//repos//TextAnalytics4Health//data//translated//LDO_AUSBO_2.pdf\n",
      "Healthcare results:\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "#Highlight PDF\r\n",
    "entities\r\n",
    "    \r\n",
    "doc = fitz.open(ORIGINAL_FOLDER+files[file_choice_num-1])\r\n",
    "\r\n",
    "for entity in health_entities:\r\n",
    "    for page in doc:\r\n",
    "        text_instances = page.searchFor(entity)\r\n",
    "        for inst in text_instances:\r\n",
    "            highlight = page.addHighlightAnnot(inst)\r\n",
    "            highlight.update()\r\n",
    "\r\n",
    "doc.save(OUTPUT_FOLDER+files[file_choice_num-1],garbage=4,deflate=True,clean=True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('text-analytics': conda)"
  },
  "interpreter": {
   "hash": "66adb518c567f42d90639443f734b1599f2bc5de9cc1f3710bddf4133289520c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}