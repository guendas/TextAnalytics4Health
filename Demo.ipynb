{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#conda activate text-analytics\r\n",
    "import os\r\n",
    "import json as js\r\n",
    "import requests, uuid\r\n",
    "import re, math\r\n",
    "from datetime import datetime, timedelta\r\n",
    "from docx import Document\r\n",
    "from docx.enum.text import WD_COLOR_INDEX\r\n",
    "from azure.core.credentials import AzureKeyCredential\r\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
    "from azure.ai.translation.document import DocumentTranslationClient\r\n",
    "from azure.storage.blob import BlobServiceClient\r\n",
    "from azure.storage.blob._shared_access_signature import BlobSharedAccessSignature\r\n",
    "\r\n",
    "#Loads info from config file \r\n",
    "with open('./config.json','r') as file:\r\n",
    "    config = js.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Local helpers\r\n",
    "def FilesInFolder(path):\r\n",
    "    file_list = os.listdir(path)\r\n",
    "    return file_list\r\n",
    "\r\n",
    "def ExtractTextFromLocal(docPath):\r\n",
    "    doc = Document(docPath)\r\n",
    "    full_text = []\r\n",
    "    for t in doc.paragraphs:\r\n",
    "        full_text.append(t.text)\r\n",
    "    return full_text\r\n",
    "\r\n",
    "def EntitySearch(document,keyword,dstPath):\r\n",
    "    for p in document.paragraphs:\r\n",
    "        if keyword in p.text:\r\n",
    "            for run in p.runs:\r\n",
    "                if keyword in run.text:\r\n",
    "                    temp = run.text.split(keyword)\r\n",
    "                    run.clear()\r\n",
    "                    for i in range(len(temp)-1):\r\n",
    "                        #run.add_text(temp[i])\r\n",
    "                        run.add_text(keyword)\r\n",
    "                        run.font.highlight_color = WD_COLOR_INDEX.YELLOW\r\n",
    "    document.save(dstPath)\r\n",
    "\r\n",
    "def ChunkText(text):\r\n",
    "    max_size = math.ceil(len(text)/10)\r\n",
    "    chunks_text = []\r\n",
    "    for x in range(0,len(text),max_size):\r\n",
    "        temp = \"\"\r\n",
    "        for t in range(x,x+max_size,x+1):\r\n",
    "            temp = temp + \" \"+ text[t]   \r\n",
    "        chunks_text.append(temp)\r\n",
    "    return chunks_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Blob helpers\r\n",
    "def ConnectToBlobStorage(connectionString):\r\n",
    "    #connects to the Azure Storage Account\r\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connectionString)\r\n",
    "    return blob_service_client\r\n",
    "\r\n",
    "def GetContainersInStorage(blob_service_client):\r\n",
    "    container_list = blob_service_client.list_containers()\r\n",
    "    return container_list\r\n",
    "\r\n",
    "def GetBlobsInContainer(blobClient,containerName):\r\n",
    "    #gets the container you want to list\r\n",
    "    container_client = blobClient.get_container_client(containerName)\r\n",
    "    #gets the blobs\r\n",
    "    blobs_list = container_client.list_blobs()\r\n",
    "    return blobs_list\r\n",
    "\r\n",
    "def DownloadBlobLocally(blobServiceClient,containerName,fileName,downloadPath):\r\n",
    "    #creates blob client\r\n",
    "    blob_client = blobServiceClient.get_blob_client(container=containerName,blob=fileName)\r\n",
    "    print(\"Downloading from Azure Storage as blob: \" + fileName)\r\n",
    "    #downloads file\r\n",
    "    download_file_path= downloadPath + fileName\r\n",
    "    with open(download_file_path,\"wb\") as download_file:\r\n",
    "        download_file.write(blob_client.download_blob().readall())\r\n",
    "    print(\"Blob downloaded in the following folder: \"+download_file_path)\r\n",
    "    return download_file_path\r\n",
    "\r\n",
    "def UploadFileToBlob(blob_service_client,containerName,fileName,filePath):\r\n",
    "    #creates blob client\r\n",
    "    blob_client = blob_service_client.get_blob_client(container=containerName,blob=fileName)  \r\n",
    "    print(\"Uploading to Azure Storage as blob: \" + fileName)\r\n",
    "    #uploads file\r\n",
    "    with open(filePath,\"rb\") as upload_file:\r\n",
    "        blob_client.upload_blob(upload_file)\r\n",
    "    print(\"Blob uploaded!\")\r\n",
    "\r\n",
    "def GetBlobURL(blob_service_client,blob_name,container_name):\r\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name,blob=blob_name)\r\n",
    "    blob_url = blob_client.url\r\n",
    "    return blob_url\r\n",
    "\r\n",
    "def CreateSASSignature(container_name, blob_name, permissions,expiry):\r\n",
    "    blob_shared_access_signature = BlobSharedAccessSignature(config[\"sa_account_name\"],config[\"sa_key\"])\r\n",
    "    sas_token = blob_shared_access_signature.generate_blob(container_name,blob_name,expiry=expiry,permission=\"rw\")\r\n",
    "    return sas_token\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Text Analytics helpers\r\n",
    "def LanguageDetection(text_analytics_client,full_text):\r\n",
    "    language_detected = text_analytics_client.detect_language(full_text)\r\n",
    "    print(\"Language detected: {}\".format(language_detected[0].primary_language))\r\n",
    "    return language_detected\r\n",
    "\r\n",
    "def DocumentTranslation(transaltor_client,source_url,target_url,language):\r\n",
    "    poller = transaltor_client.begin_translation(source_url,target_url,language)\r\n",
    "    result = poller.result()\r\n",
    "    #print(\"Status: {}\".format(poller.status()))\r\n",
    "    #print(\"Created on: {}\".format(poller.details.created_on))\r\n",
    "    #print(\"Last updated on: {}\".format(poller.details.last_updated_on))\r\n",
    "    #print(\"Total number of translations on documents: {}\".format(poller.details.documents_total_count))\r\n",
    "    for document in result:\r\n",
    "        print(\"Document ID: {}\".format(document.id))\r\n",
    "        print(\"Document status: {}\".format(document.status))\r\n",
    "        if document.status == \"Succeeded\":\r\n",
    "            print(\"Source document location: {}\".format(document.source_document_url))\r\n",
    "            print(\"Translated document location: {}\".format(document.translated_document_url))\r\n",
    "            print(\"Translated to language: {}\\n\".format(document.translated_to))\r\n",
    "            return document.translated_document_url\r\n",
    "        else:\r\n",
    "            print(\"Error Code: {}, Message: {}\\n\".format(document.error.code, document.error.message))\r\n",
    "\r\n",
    "def TextAnalyticsForHealth(text_analytics_client,text):\r\n",
    "    poller = text_analytics_client.begin_analyze_healthcare_entities(text)\r\n",
    "    result = poller.result()\r\n",
    "    docs = [doc for doc in result if not doc.is_error]\r\n",
    "    print(\"Healthcare results:\")\r\n",
    "    # for idx, doc in enumerate(docs):\r\n",
    "    #     for entity in doc.entities:\r\n",
    "    #         print(\"Entity: {}\".format(entity.text))\r\n",
    "    #         print(\"...Normalized Text: {}\".format(entity.normalized_text))\r\n",
    "    #         print(\"...Category: {}\".format(entity.category))\r\n",
    "    #         print(\"...Subcategory: {}\".format(entity.subcategory))\r\n",
    "    #         print(\"...Offset: {}\".format(entity.offset))\r\n",
    "    #         print(\"...Confidence score: {}\".format(entity.confidence_score))\r\n",
    "    #         if entity.data_sources is not None:\r\n",
    "    #             print(\"...Data Sources:\")\r\n",
    "    #             for data_source in entity.data_sources:\r\n",
    "    #                 print(\"......Entity ID: {}\".format(data_source.entity_id))\r\n",
    "    #                 print(\"......Name: {}\".format(data_source.name))\r\n",
    "    #         if entity.assertion is not None:\r\n",
    "    #             print(\"...Assertion:\")\r\n",
    "    #             print(\"......Conditionality: {}\".format(entity.assertion.conditionality))\r\n",
    "    #             print(\"......Certainty: {}\".format(entity.assertion.certainty))\r\n",
    "    #             print(\"......Association: {}\".format(entity.assertion.association))\r\n",
    "    #     for relation in doc.entity_relations:\r\n",
    "    #         print(\"Relation of type: {} has the following roles\".format(relation.relation_type))\r\n",
    "    #         for role in relation.roles:\r\n",
    "    #             print(\"...Role '{}' with entity '{}'\".format(role.name, role.entity.text))\r\n",
    "    #     print(\"------------------------------------------\")\r\n",
    "    return docs\r\n",
    "\r\n",
    "def TextAnalyticsPII(text_analytics_client,text,language):\r\n",
    "    response = text_analytics_client.recognize_pii_entities(text, language=language)\r\n",
    "    result = [doc for doc in response if not doc.is_error]\r\n",
    "    for idx, doc in enumerate(result):\r\n",
    "        #print(\"Document text: {}\".format(documents[idx]))\r\n",
    "        #print(\"Redacted document text: {}\".format(doc.redacted_text))\r\n",
    "        for entity in doc.entities:\r\n",
    "            print(\"...Entity: {}\".format(entity.text))\r\n",
    "            print(\"......Category: {}\".format(entity.category))\r\n",
    "            print(\"......Confidence Score: {}\".format(entity.confidence_score))\r\n",
    "            print(\"......Offset: {}\".format(entity.offset))\r\n",
    "\r\n",
    "def TextAnalyticsOnDocs(docs_folder,translated_folder,ta_client,tr_client,blob_client,container_name,source_url,target_url):\r\n",
    "    #Check files locally \r\n",
    "    file_list = FilesInFolder(docs_folder)\r\n",
    "    for f in file_list:\r\n",
    "        print(\"File processed {}\".format(f))\r\n",
    "        #Stores file path\r\n",
    "        file_path = docs_folder + f\r\n",
    "        #Extract file text\r\n",
    "        extracted_text = ExtractTextFromLocal(file_path)\r\n",
    "        ########################\r\n",
    "        #       STEP 1         #\r\n",
    "        #                      #\r\n",
    "        #  LANGUAGE DETECTION  #\r\n",
    "        ########################\r\n",
    "        language_detected = LanguageDetection(ta_client,extracted_text)\r\n",
    "        ########################\r\n",
    "        #       STEP 2         #\r\n",
    "        #                      #\r\n",
    "        # DOCUMENT TRANSLATION #\r\n",
    "        ########################\r\n",
    "        #Upload file to Blob storage\r\n",
    "        UploadFileToBlob(blob_client,container_name,f,file_path)\r\n",
    "        #Translate document\r\n",
    "        translated_doc_url = DocumentTranslation(tr_client,source_url,target_url,\"en\")\r\n",
    "        #################################\r\n",
    "        #             STEP 3            #\r\n",
    "        #                               #\r\n",
    "        # DOWNLOAD AND EXTRACT ENTITIES #\r\n",
    "        #################################\r\n",
    "        #Download file \r\n",
    "        translated_file_path = DownloadBlobLocally(blob_client,\"output\",f,translated_folder)\r\n",
    "        fully_translated_text = ExtractTextFromLocal(translated_file_path)\r\n",
    "        #Format text to be sent to the TA\r\n",
    "        ct = ChunkText(fully_translated_text)\r\n",
    "        #Extract entities\r\n",
    "        entities = TextAnalyticsForHealth(ta_client,ct)\r\n",
    "        #################################\r\n",
    "        #             STEP 4            #\r\n",
    "        #                               #\r\n",
    "        #   HIGHLIGHT ENTITIES IN DOCX  #\r\n",
    "        #################################\r\n",
    "        translated_document = Document(translated_file_path)\r\n",
    "        for d in entities:\r\n",
    "            for entity in d.entities:\r\n",
    "                print(entity.text)\r\n",
    "                EntitySearch(translated_document,entity.text,\"ExtractedEntities.docx\")\r\n",
    "        #################################\r\n",
    "        #             STEP 5            #\r\n",
    "        #                               #\r\n",
    "        #              PII              #\r\n",
    "        #################################\r\n",
    "        TextAnalyticsPII(ta_client,fully_translated_text,\"en\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Helpers\r\n",
    "DOCS_FOLDER = config[\"preprocessed_path\"]\r\n",
    "ORIGINAL_FOLDER =config[\"pdfs_path\"]\r\n",
    "OUTPUT_FOLDER = config[\"output_path\"]\r\n",
    "TRANSLATED_FOLDER = config[\"translated_path\"]\r\n",
    "SOURCE_URL = config[\"sa_source_url\"]\r\n",
    "TARGET_URL = config[\"sa_target_url\"]\r\n",
    "\r\n",
    "#Connect to Azure Text Analytics service\r\n",
    "ta_credentials = AzureKeyCredential(config[\"text_analytics_key\"])\r\n",
    "TEXT_ANALYTICS_CLIENT = TextAnalyticsClient(endpoint=config[\"text_analytics_endpoint\"],credential=ta_credentials)\r\n",
    "\r\n",
    "#Connect to Azure Translator service\r\n",
    "tr_credentials = AzureKeyCredential(config[\"translator_key\"])\r\n",
    "TRANSLATOR_CLIENT= DocumentTranslationClient(endpoint=config[\"translator_documents_endpoint\"],credential=tr_credentials)\r\n",
    "\r\n",
    "#Blob storage connection\r\n",
    "BLOBSERVICECLIENT = ConnectToBlobStorage(config[\"sa_connectionstring\"])\r\n",
    "CONTAINER_NAME = \"data\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "TextAnalyticsOnDocs(DOCS_FOLDER,TRANSLATED_FOLDER,TEXT_ANALYTICS_CLIENT,TRANSLATOR_CLIENT,BLOBSERVICECLIENT,CONTAINER_NAME,SOURCE_URL,TARGET_URL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File processed LDO_AOPR_1.docx\n",
      "Language detected: {'name': 'English', 'iso6391_name': 'en', 'confidence_score': 1.0}\n",
      "Uploading to Azure Storage as blob: LDO_AOPR_1.docx\n",
      "Blob uploaded!\n",
      "Document ID: 0007e980-0000-0000-0000-000000000000\n",
      "Document status: Succeeded\n",
      "Source document location: https://guscianchealthsa.blob.core.windows.net/data/LDO_AOPR_1.docx\n",
      "Translated document location: https://guscianchealthsa.blob.core.windows.net/output/LDO_AOPR_1.docx\n",
      "Translated to language: en\n",
      "\n",
      "Downloading from Azure Storage as blob: LDO_AOPR_1.docx\n",
      "Blob downloaded in the following folder: C://Users//guscianc//source//repos//TextAnalytics//data//translated//LDO_AOPR_1.docx\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "HttpResponseError",
     "evalue": "(InvalidDocumentBatch) Batch request contains too many records. Max 10 records are permitted.\nCode: InvalidDocumentBatch\nMessage: Batch request contains too many records. Max 10 records are permitted.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\envs\\text-analytics\\lib\\site-packages\\azure\\ai\\textanalytics\\_text_analytics_client.py\u001b[0m in \u001b[0;36mbegin_analyze_healthcare_entities\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m             return self._client.begin_health(\n\u001b[0m\u001b[0;32m    613\u001b[0m                 \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\text-analytics\\lib\\site-packages\\azure\\ai\\textanalytics\\_generated\\_operations_mixin.py\u001b[0m in \u001b[0;36mbegin_health\u001b[1;34m(self, documents, model_version, string_index_type, logging_opt_out, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mmixin_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deserialize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeserializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_models_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmixin_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_health\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_index_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_opt_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\text-analytics\\lib\\site-packages\\azure\\ai\\textanalytics\\_generated\\v3_1\\operations\\_text_analytics_client_operations.py\u001b[0m in \u001b[0;36mbegin_health\u001b[1;34m(self, documents, model_version, string_index_type, logging_opt_out, **kwargs)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcont_token\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m             raw_result = self._health_initial(\n\u001b[0m\u001b[0;32m    525\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\text-analytics\\lib\\site-packages\\azure\\ai\\textanalytics\\_generated\\v3_1\\operations\\_text_analytics_client_operations.py\u001b[0m in \u001b[0;36m_health_initial\u001b[1;34m(self, documents, model_version, string_index_type, logging_opt_out, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailsafe_deserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mErrorResponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHttpResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (InvalidRequest) Invalid document in request.\nCode: InvalidRequest\nMessage: Invalid document in request.\nInner error: {\n    \"code\": \"InvalidDocumentBatch\",\n    \"message\": \"Batch request contains too many records. Max 10 records are permitted.\"\n}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bf6f80156f00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTextAnalyticsOnDocs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-4ef4c7fd706f>\u001b[0m in \u001b[0;36mTextAnalyticsOnDocs\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mfully_translated_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtractTextFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslated_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m#Extract entities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextAnalyticsForHealth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEXT_ANALYTICS_CLIENT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfully_translated_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;31m#             STEP 4            #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;31m#                               #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-4ef4c7fd706f>\u001b[0m in \u001b[0;36mTextAnalyticsForHealth\u001b[1;34m(text_analytics_client, text)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mTextAnalyticsForHealth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_analytics_client\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mpoller\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_analytics_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_analyze_healthcare_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\text-analytics\\lib\\site-packages\\azure\\core\\tracing\\decorator.py\u001b[0m in \u001b[0;36mwrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mspan_impl_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracing_implementation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspan_impl_type\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\text-analytics\\lib\\site-packages\\azure\\ai\\textanalytics\\_text_analytics_client.py\u001b[0m in \u001b[0;36mbegin_analyze_healthcare_entities\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHttpResponseError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m             \u001b[0mprocess_http_response_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdistributed_trace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\text-analytics\\lib\\site-packages\\azure\\ai\\textanalytics\\_response_handlers.py\u001b[0m in \u001b[0;36mprocess_http_response_error\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m401\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mraise_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClientAuthenticationError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCSODataV4Format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (InvalidDocumentBatch) Batch request contains too many records. Max 10 records are permitted.\nCode: InvalidDocumentBatch\nMessage: Batch request contains too many records. Max 10 records are permitted."
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}