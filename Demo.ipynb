{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#conda activate text-analytics\r\n",
    "\r\n",
    "import os\r\n",
    "import json as js\r\n",
    "import requests, uuid\r\n",
    "import re, math\r\n",
    "from datetime import datetime, timedelta\r\n",
    "from docx import Document\r\n",
    "from docx.enum.text import WD_COLOR_INDEX\r\n",
    "from azure.core.credentials import AzureKeyCredential\r\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\r\n",
    "from azure.ai.translation.document import DocumentTranslationClient\r\n",
    "from azure.storage.blob import BlobServiceClient\r\n",
    "from azure.storage.blob._shared_access_signature import BlobSharedAccessSignature\r\n",
    "\r\n",
    "#Loads info from config file \r\n",
    "with open('./config.json','r') as file:\r\n",
    "    config = js.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Demo setup </h1>\r\n",
    "\r\n",
    "Before running this demo you need few things set up: \r\n",
    "\r\n",
    "<h4>Azure Resources</h4>\r\n",
    "<ul>\r\n",
    "<li> [Azure Text Analytics] (https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/overview) </li>\r\n",
    "<li> [Azure Translator] (https://docs.microsoft.com/en-us/azure/cognitive-services/translator/document-translation/get-started-with-document-translation?WT.mc_id=Portal-Microsoft_Azure_ProjectOxford&tabs=csharp) </li>\r\n",
    "<ul>\r\n",
    "<li>Create a SAS token policy to use Document Translator [here] (https://docs.microsoft.com/en-us/azure/cognitive-services/translator/document-translation/create-sas-tokens?tabs=Containers)</li>\r\n",
    "</ul>\r\n",
    "</ul>\r\n",
    "\r\n",
    "<h4> Folder Structure </h4>\r\n",
    "In the root folder:\r\n",
    "<ul>\r\n",
    "<li> \"data\" folder, inside the data folder: </li>\r\n",
    "<ul>\r\n",
    "<li> \"original\" folder (where your original pdf files will be)</li>\r\n",
    "<li> \"output\" folder (where your outputted highlighted docx file will be stored)</li>\r\n",
    "<li> \"preprocessed\" folder (where your docx file will be)</li>\r\n",
    "<li> \"translated\" folder (where you can download your translated docx)</li>\r\n",
    "</ul>\r\n",
    "<li> \"config.json\" file, with all the path and keys </li>\r\n",
    "<li> notebook </li>\r\n",
    "</ul>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Helpers functions definition </h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Local helpers\r\n",
    "\r\n",
    "################################\r\n",
    "# Lists files in folder        #\r\n",
    "################################\r\n",
    "def FilesInFolder(path):\r\n",
    "    file_list = os.listdir(path)\r\n",
    "    return file_list\r\n",
    "\r\n",
    "################################\r\n",
    "#Extracts text from docx file  #\r\n",
    "################################\r\n",
    "def ExtractTextFromLocal(docPath):\r\n",
    "    doc = Document(docPath)\r\n",
    "    full_text = []\r\n",
    "    for t in doc.paragraphs:\r\n",
    "        full_text.append(t.text)\r\n",
    "    return full_text\r\n",
    "\r\n",
    "##############################################\r\n",
    "# Highlights entities found in the docx file #\r\n",
    "##############################################\r\n",
    "def EntitySearch(document,keyword,dstPath,color):\r\n",
    "    for p in document.paragraphs:\r\n",
    "        if keyword in p.text:\r\n",
    "            for run in p.runs:\r\n",
    "                if keyword in run.text:\r\n",
    "                    temp = run.text.split(keyword)\r\n",
    "                    run.clear()\r\n",
    "                    for i in range(len(temp)-1):\r\n",
    "                        #run.add_text(temp[i])\r\n",
    "                        run.add_text(keyword)\r\n",
    "                        if(color == 'y'):\r\n",
    "                            run.font.highlight_color = WD_COLOR_INDEX.YELLOW\r\n",
    "                        else:\r\n",
    "                            run.font.highlight_color = WD_COLOR_INDEX.GREEN\r\n",
    "    document.save(dstPath)\r\n",
    "\r\n",
    "################################################\r\n",
    "# Takes extracted document text and divides it #\r\n",
    "# in max n parts to avoid API limits           #\r\n",
    "################################################\r\n",
    "def ChunkText(text,n):\r\n",
    "    max_size = math.ceil(len(text)/n)\r\n",
    "    chunks_text = []\r\n",
    "    for x in range(0,len(text),max_size):\r\n",
    "        temp = \"\"\r\n",
    "        for t in range(x,x+max_size,x+1):\r\n",
    "            temp = temp + \" \"+ text[t]   \r\n",
    "        chunks_text.append(temp)\r\n",
    "    return chunks_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Blob helpers\r\n",
    "\r\n",
    "######################################\r\n",
    "# Creates connection to blob storage #\r\n",
    "######################################\r\n",
    "def ConnectToBlobStorage(connectionString):\r\n",
    "    #connects to the Azure Storage Account\r\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connectionString)\r\n",
    "    return blob_service_client\r\n",
    "\r\n",
    "###################################\r\n",
    "# Lists container in blob storage #\r\n",
    "###################################\r\n",
    "def GetContainersInStorage(blob_service_client):\r\n",
    "    container_list = blob_service_client.list_containers()\r\n",
    "    return container_list\r\n",
    "\r\n",
    "#####################################\r\n",
    "# Lists blobs in specific container #\r\n",
    "#####################################\r\n",
    "def GetBlobsInContainer(blobClient,containerName):\r\n",
    "    #gets the container you want to list\r\n",
    "    container_client = blobClient.get_container_client(containerName)\r\n",
    "    #gets the blobs\r\n",
    "    blobs_list = container_client.list_blobs()\r\n",
    "    return blobs_list\r\n",
    "\r\n",
    "###############################################\r\n",
    "# Downloads locally specific blob in storage  #\r\n",
    "###############################################\r\n",
    "def DownloadBlobLocally(blobServiceClient,containerName,fileName,downloadPath):\r\n",
    "    #creates blob client\r\n",
    "    blob_client = blobServiceClient.get_blob_client(container=containerName,blob=fileName)\r\n",
    "    print(\"Downloading from Azure Storage as blob: \" + fileName)\r\n",
    "    #downloads file\r\n",
    "    download_file_path= downloadPath + fileName\r\n",
    "    with open(download_file_path,\"wb\") as download_file:\r\n",
    "        download_file.write(blob_client.download_blob().readall())\r\n",
    "    print(\"Blob downloaded in the following folder: \"+download_file_path)\r\n",
    "    return download_file_path\r\n",
    "\r\n",
    "########################################################\r\n",
    "# Uploads local file to specific container in storage  #\r\n",
    "########################################################\r\n",
    "def UploadFileToBlob(blob_service_client,containerName,fileName,filePath):\r\n",
    "    #creates blob client\r\n",
    "    blob_client = blob_service_client.get_blob_client(container=containerName,blob=fileName)  \r\n",
    "    print(\"Uploading to Azure Storage as blob: \" + fileName)\r\n",
    "    #uploads file\r\n",
    "    with open(filePath,\"rb\") as upload_file:\r\n",
    "        blob_client.upload_blob(upload_file)\r\n",
    "    print(\"Blob uploaded!\")\r\n",
    "\r\n",
    "##########################################\r\n",
    "# Gets blob url from specific container  #\r\n",
    "##########################################\r\n",
    "def GetBlobURL(blob_service_client,blob_name,container_name):\r\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name,blob=blob_name)\r\n",
    "    blob_url = blob_client.url\r\n",
    "    return blob_url\r\n",
    "\r\n",
    "###########################################\r\n",
    "# Creates SAS signature for specific blob #\r\n",
    "###########################################\r\n",
    "def CreateSASSignature(container_name, blob_name, permissions,expiry):\r\n",
    "    blob_shared_access_signature = BlobSharedAccessSignature(config[\"sa_account_name\"],config[\"sa_key\"])\r\n",
    "    sas_token = blob_shared_access_signature.generate_blob(container_name,blob_name,expiry=expiry,permission=\"rw\")\r\n",
    "    return sas_token\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Text Analytics helpers\r\n",
    "\r\n",
    "#############################\r\n",
    "# Detects document language #\r\n",
    "#############################\r\n",
    "def LanguageDetection(text_analytics_client,full_text):\r\n",
    "    language_detected = text_analytics_client.detect_language(full_text)\r\n",
    "    print(\"Language detected: {}\".format(language_detected[0].primary_language))\r\n",
    "    return language_detected\r\n",
    "\r\n",
    "############################################\r\n",
    "# Translates documents online (from blob)  #\r\n",
    "############################################\r\n",
    "def DocumentTranslation(transaltor_client,source_url,target_url,language):\r\n",
    "    poller = transaltor_client.begin_translation(source_url,target_url,language)\r\n",
    "    result = poller.result()\r\n",
    "    #print(\"Status: {}\".format(poller.status()))\r\n",
    "    #print(\"Created on: {}\".format(poller.details.created_on))\r\n",
    "    #print(\"Last updated on: {}\".format(poller.details.last_updated_on))\r\n",
    "    #print(\"Total number of translations on documents: {}\".format(poller.details.documents_total_count))\r\n",
    "    for document in result:\r\n",
    "        print(\"Document ID: {}\".format(document.id))\r\n",
    "        print(\"Document status: {}\".format(document.status))\r\n",
    "        if document.status == \"Succeeded\":\r\n",
    "            print(\"Source document location: {}\".format(document.source_document_url))\r\n",
    "            print(\"Translated document location: {}\".format(document.translated_document_url))\r\n",
    "            print(\"Translated to language: {}\\n\".format(document.translated_to))\r\n",
    "            return document.translated_document_url\r\n",
    "        else:\r\n",
    "            print(\"Error Code: {}, Message: {}\\n\".format(document.error.code, document.error.message))\r\n",
    "\r\n",
    "#######################################\r\n",
    "# Extracts health entities from text  #\r\n",
    "#######################################\r\n",
    "def TextAnalyticsForHealth(text_analytics_client,text):\r\n",
    "    poller = text_analytics_client.begin_analyze_healthcare_entities(text)\r\n",
    "    result = poller.result()\r\n",
    "    health_entities = []\r\n",
    "    docs = [doc for doc in result if not doc.is_error]\r\n",
    "    print(\"Healthcare results:\")\r\n",
    "    for idx, doc in enumerate(docs):\r\n",
    "        for entity in doc.entities:\r\n",
    "            health_entities.append(entity.text)\r\n",
    "    #         print(\"Entity: {}\".format(entity.text))\r\n",
    "    #         print(\"...Normalized Text: {}\".format(entity.normalized_text))\r\n",
    "    #         print(\"...Category: {}\".format(entity.category))\r\n",
    "    #         print(\"...Subcategory: {}\".format(entity.subcategory))\r\n",
    "    #         print(\"...Offset: {}\".format(entity.offset))\r\n",
    "    #         print(\"...Confidence score: {}\".format(entity.confidence_score))\r\n",
    "    #         if entity.data_sources is not None:\r\n",
    "    #             print(\"...Data Sources:\")\r\n",
    "    #             for data_source in entity.data_sources:\r\n",
    "    #                 print(\"......Entity ID: {}\".format(data_source.entity_id))\r\n",
    "    #                 print(\"......Name: {}\".format(data_source.name))\r\n",
    "    #         if entity.assertion is not None:\r\n",
    "    #             print(\"...Assertion:\")\r\n",
    "    #             print(\"......Conditionality: {}\".format(entity.assertion.conditionality))\r\n",
    "    #             print(\"......Certainty: {}\".format(entity.assertion.certainty))\r\n",
    "    #             print(\"......Association: {}\".format(entity.assertion.association))\r\n",
    "    #     for relation in doc.entity_relations:\r\n",
    "    #         print(\"Relation of type: {} has the following roles\".format(relation.relation_type))\r\n",
    "    #         for role in relation.roles:\r\n",
    "    #             print(\"...Role '{}' with entity '{}'\".format(role.name, role.entity.text))\r\n",
    "    #     print(\"------------------------------------------\")\r\n",
    "    return docs,health_entities\r\n",
    "\r\n",
    "###########################\r\n",
    "# Extracts PII from text  #\r\n",
    "###########################\r\n",
    "def TextAnalyticsPII(text_analytics_client,text,language):\r\n",
    "    response = text_analytics_client.recognize_pii_entities(text, language=language)\r\n",
    "    result = [doc for doc in response if not doc.is_error]\r\n",
    "    pii_entities = []\r\n",
    "    for idx, doc in enumerate(result):\r\n",
    "        #print(\"Document text: {}\".format(documents[idx]))\r\n",
    "        #print(\"Redacted document text: {}\".format(doc.redacted_text))\r\n",
    "        for entity in doc.entities:\r\n",
    "            pii_entities.append(entity.text)\r\n",
    "            #print(\"...Entity: {}\".format(entity.text))\r\n",
    "            #print(\"......Category: {}\".format(entity.category))\r\n",
    "            #print(\"......Confidence Score: {}\".format(entity.confidence_score))\r\n",
    "            #print(\"......Offset: {}\".format(entity.offset))\r\n",
    "    return result,pii_entities\r\n",
    "\r\n",
    "#############################################\r\n",
    "# Execute all the steps to: extracts text   #\r\n",
    "# from docx, translates it, extracts health #\r\n",
    "#    entities, highlights them in docx,     #\r\n",
    "#    extracts PII entities from docx        #\r\n",
    "#############################################\r\n",
    "def TextAnalyticsOnDocs(docs_folder,translated_folder,output_folder,ta_client,tr_client,blob_client,container_name,source_url,target_url):\r\n",
    "    #Check files locally \r\n",
    "    file_list = FilesInFolder(docs_folder)\r\n",
    "    for f in file_list:\r\n",
    "        print(\"File processed {}\".format(f))\r\n",
    "        #Stores file path\r\n",
    "        file_path = docs_folder + f\r\n",
    "        #Extract file text\r\n",
    "        extracted_text = ExtractTextFromLocal(file_path)\r\n",
    "        ########################\r\n",
    "        #       STEP 1         #\r\n",
    "        #                      #\r\n",
    "        #  LANGUAGE DETECTION  #\r\n",
    "        ########################\r\n",
    "        language_detected = LanguageDetection(ta_client,extracted_text)\r\n",
    "        ########################\r\n",
    "        #       STEP 2         #\r\n",
    "        #                      #\r\n",
    "        # DOCUMENT TRANSLATION #\r\n",
    "        ########################\r\n",
    "        #Upload file to Blob storage\r\n",
    "        UploadFileToBlob(blob_client,container_name,f,file_path)\r\n",
    "        #Translate document\r\n",
    "        translated_doc_url = DocumentTranslation(tr_client,source_url,target_url,\"en\")\r\n",
    "        #################################\r\n",
    "        #             STEP 3            #\r\n",
    "        #                               #\r\n",
    "        # DOWNLOAD AND EXTRACT ENTITIES #\r\n",
    "        #################################\r\n",
    "        #Download file \r\n",
    "        translated_file_path = DownloadBlobLocally(blob_client,\"output\",f,translated_folder)\r\n",
    "        fully_translated_text = ExtractTextFromLocal(translated_file_path)\r\n",
    "        #Format text to be sent to the TA\r\n",
    "        ct = ChunkText(fully_translated_text,n)\r\n",
    "        #Extract entities\r\n",
    "        entities, health_entities = TextAnalyticsForHealth(ta_client,ct)\r\n",
    "        #################################\r\n",
    "        #             STEP 4            #\r\n",
    "        #                               #\r\n",
    "        #   HIGHLIGHT ENTITIES IN DOCX  #\r\n",
    "        #################################\r\n",
    "        translated_document = Document(translated_file_path)\r\n",
    "        #Highlighted doc path\r\n",
    "        highlighted_doc_path = output_folder + f\r\n",
    "        for d in entities:\r\n",
    "            for entity in d.entities:\r\n",
    "                print(entity.text)\r\n",
    "                EntitySearch(translated_document,entity.text,highlighted_doc_path,'y')\r\n",
    "        #################################\r\n",
    "        #             STEP 5            #\r\n",
    "        #                               #\r\n",
    "        #              PII              #\r\n",
    "        #################################\r\n",
    "        pii_extraction = ChunkText(fully_translated_text,5)\r\n",
    "        results,pii_entities = TextAnalyticsPII(ta_client,pii_extraction,\"en\")\r\n",
    "        #################################\r\n",
    "        #             STEP 6            #\r\n",
    "        #                               #\r\n",
    "        #         HIGHLIGHT PII         #\r\n",
    "        #################################\r\n",
    "        for r in results:\r\n",
    "            for entity in r.entities:\r\n",
    "                #Uncomment to show results\r\n",
    "                #print(entity.text)\r\n",
    "                EntitySearch(translated_document,entity.text,highlighted_doc_path,'g')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Code to run Text Analytics for Health on specific document </h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Helpers\r\n",
    "DOCS_FOLDER = config[\"preprocessed_path\"]\r\n",
    "ORIGINAL_FOLDER =config[\"pdfs_path\"]\r\n",
    "OUTPUT_FOLDER = config[\"output_path\"]\r\n",
    "TRANSLATED_FOLDER = config[\"translated_path\"]\r\n",
    "SOURCE_URL = config[\"sa_source_url\"]\r\n",
    "TARGET_URL = config[\"sa_target_url\"]\r\n",
    "\r\n",
    "#Connect to Azure Text Analytics service\r\n",
    "ta_credentials = AzureKeyCredential(config[\"text_analytics_key\"])\r\n",
    "TEXT_ANALYTICS_CLIENT = TextAnalyticsClient(endpoint=config[\"text_analytics_endpoint\"],credential=ta_credentials)\r\n",
    "\r\n",
    "#Connect to Azure Translator service\r\n",
    "tr_credentials = AzureKeyCredential(config[\"translator_key\"])\r\n",
    "TRANSLATOR_CLIENT= DocumentTranslationClient(endpoint=config[\"translator_documents_endpoint\"],credential=tr_credentials)\r\n",
    "\r\n",
    "#Blob storage connection\r\n",
    "BLOBSERVICECLIENT = ConnectToBlobStorage(config[\"sa_connectionstring\"])\r\n",
    "#Hardcoded container name\r\n",
    "CONTAINER_NAME = \"data\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Analyze docx with Azure Text Analytics for Health\r\n",
    "TextAnalyticsOnDocs(DOCS_FOLDER,TRANSLATED_FOLDER,OUTPUT_FOLDER,TEXT_ANALYTICS_CLIENT,TRANSLATOR_CLIENT,BLOBSERVICECLIENT,CONTAINER_NAME,SOURCE_URL,TARGET_URL)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}